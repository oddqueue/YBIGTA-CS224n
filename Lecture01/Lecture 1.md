# Lecture 1

### Introductions and Word Vectors

---

### ê°•ì¢Œ ì†Œê°œ

- NLPì— ì‚¬ìš©ë˜ëŠ” ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆì— ëŒ€í•œ ì´í•´
- Basic Model â†’ RNN â†’ Attention â†’ Transformers ìˆœì„œ
- ***Pytorch*** Implementation ìˆ˜í–‰

---

### ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ë²•

- **ê¸°ì˜ì™€ ê¸°í‘œ**
    - signifier(symbol) â†” signified(idea or thing, concept)
    - denotational semantics
- **WordNet : ê³ ì „ì ì¸ NLP í•´ê²°ì±…**
    - ë™ì˜ì–´ì™€ ìƒì˜ì–´ ë“±ì˜ listë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” thesaurus(ì‚¬ì „)
    
<aside>
ğŸ’¡ WordNet-like resourcesì˜ ì·¨ì•½ì 

- ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¦„ : missing nuance
- ìƒˆë¡œìš´ ì˜ë¯¸ ë“±ì¥(ì‹ ì¡°ì–´)
- ì£¼ê´€ì ì¸ ë¶„ë¥˜ ê¸°ì¤€, requires human labor
- word similarityì˜ ì •ëŸ‰ì  ê°œë… ë¶€ì¬
</aside>
    
- **ë‹¨ì–´ë¥¼ ì›í•« ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸°**
    - all non-identical word vectors are orthogonal
    - vector dimension = vocabularyì— ì†í•œ ëª¨ë“  ë‹¨ì–´ì˜ ê°œìˆ˜
    - **ì·¨ì•½ì **
        - word similarityì˜ ì •ëŸ‰ì  ê°œë… ë¶€ì¬
- **ë¶„ë°° ê°€ì„¤(distributional semantics, hypothesis)**
    - ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ì¸ì ‘í•˜ê²Œ ìœ„ì¹˜í•œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì˜í•´ ê²°ì •ëœë‹¤.
    - ë¬¸ë§¥(context) := ê·¼ì²˜ì— ë“±ì¥í•˜ëŠ” wordsì˜ ì§‘í•©
    - ***context of w builds a representation of w***
    - ë‹¨ì–´ ë²¡í„° â†’ word vectors, word embeddings, neural word representations
    - n-dimensional  ë‹¨ì–´ ë²¡í„°ë“¤ì˜ distributionì„ ***principal axis PC1, PC2***ì— ëŒ€í•´ projection
        - 2D scatter plotì²˜ëŸ¼ ë‚˜íƒ€ë‚¨
        - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ë¼ë¦¬ clusterë˜ëŠ” ê²½í–¥

---

### Word2vec (Mikolov et al. 2013)

<aside>
ğŸ’¡ Word2vecì˜ ì•„ì´ë””ì–´

- large corpus of textê°€ ì£¼ì–´ì§(typically large but rare words are truncated...)
- ê³ ì •ëœ vocabulary ë‚´ì˜ ëª¨ë“  ë‹¨ì–´ëŠ” vectorë¡œ í‘œí˜„ ê°€ëŠ¥
- í…ìŠ¤íŠ¸ì˜ position **t**ë¥¼ ìˆœíšŒí•  ë•Œ ì¤‘ì‹¬ ë‹¨ì–´(center) **c**ì™€ ë¬¸ë§¥ ë‹¨ì–´(outside) **o**ë¥¼ ì •ì˜
- **similarity of word vectors for c, o â†’ P(c|o), P(o|c) ê³„ì‚°ì— í™œìš©**
- ìœ„ì˜ Probabilityë¥¼ maximizeí•˜ëŠ” c, o word vector adjustment
</aside>

<aside>
ğŸ’¡ How Word2Vec Works

- Maximize likelihood, -log(L) = J
- Gradient Descent with Loss function
- SGD is strongly recommended due to expensive gradient calculation
- í•˜ë‚˜ì˜ window, with m=0 â†’ Gradient = observed vector - expected vector
    - observation-expectationì˜ ì°¨ì´ë§Œí¼ gradientê°€ ë‚˜íƒ€ë‚˜ê³ , learning rateë¥¼ ê³±í•œ ê²ƒë§Œí¼ parameter updateê°€ ì¼ì–´ë‚¨
</aside>

![PNG á„‹á…µá„†á…µá„Œá…µ.png](https://user-images.githubusercontent.com/75057952/150629972-abe7bf0c-8e9f-4e40-b5c2-167c7c77badb.png)

![PNG á„‹á…µá„†á…µá„Œá…µ.png](https://user-images.githubusercontent.com/75057952/150629971-ae53502e-a778-4436-8fa8-16411d03beb6.png)

**Analogy Task with Word2Vec**

- 2D plane ìœ„ì— projection
- Man : King â‡’ Woman : â“
- Vector Compositionìœ¼ë¡œ ê³„ì‚°
